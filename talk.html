<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <title>TensorFlow</title>

  <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
  <meta name="author" content="Hakim El Hattab">

  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="module" src="/src/talk.ts" defer></script>

</head>

<body>

  <div class="reveal">
    <div class="slides">
      <section data-auto-animate data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
        <a href="https://www.tensorflow.org/" target="_blank" rel="noreferrer noopener">
          <img src="/assets/logo.png" alt="TensorFlow Logo"
            style="height: 180px; margin: 0 auto 4rem auto; background: transparent;" />
        </a>
        <h2 data-id="title">TensorFlow</h2>
        <h4>open source machine learning platform</h4>
        <p>
          <small>
            by
            <a href="https://philiplehmann.ch" target="_blank" rel="noreferrer noopener">
              Philip Lehmann
            </a>
          </small>
        </p>
      </section>

      <section data-auto-animate data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
        <h2 data-id="title">Hallo Zusammen</h2>
        <ul>
          <li class="fragment">Machine Learning 101</li>
          <li class="fragment">TensorFlow</li>
          <li class="fragment">Common Objects in Context</li>
          <li class="fragment">Beispiel</li>
        </ul>
      </section>

      <section>
        <section data-auto-animate>
          <h2 data-id="title">Machine Learning</h2>
          <ul>
            <li>Zweig der künstlichen Intelligenz</li>
            <li>Idee, dass Systeme aus Daten lernen können</li>
            <li>erkennen von Mustern und treffen von Entscheidungen mit minimalem menschlichen Eingriff</li>
          </ul>
          <aside class="notes">
            primärer visueller Kortex - V1 140 mio neurons<br />
            milliarden von verbindungen<br />
            V2, V3, V4, and V5 - welche zunehmend komplexere Bildverarbeitung durchführen
          </aside>
        </section>
        <section>
          <a href="https://theosz.medium.com/120-machine-learning-business-ideas-from-the-new-mckinsey-report-b81b239f336"
            target="_blank" rel="noreferrer noopener">
            <img src="/assets/machine_learning_potential.png" alt="Machine Learning Ideas" />
            <small>https://theosz.medium.com/120-machine-...-report</small>
          </a>
        </section>
      </section>
      <section data-auto-animate>
        <h2 data-id="title">TensorFlow</h2>
        <ul>
          <li>Python (CPU, GPU, TPU)</li>
          <li>JavaScript (CPU, GPU)</li>
          <li>For Mobile & IoT (CPU, GPU, RPI)</li>
          <li>End-to-end production (CPU, GPU, TPU)</li>
        </ul>
        <p></p>
        <aside class="notes">
          CPU - Central processing unit<br />
          GPU - Graphics processing unit<br />
          TPU - Tensor Processing Unit<br />
          RPI - Raspberry Pi
        </aside>
      </section>
      <section>
        <section data-auto-animate>
          <h2 data-id="title">Common Objects in Context</h2>
          <p>COCO ist ein umfangreicher Datensatz für Objekterkennung, Segmentierung und Beschriftung</p>
          <a href="https://cocodataset.org/" target="_blank" rel="noopener noreferrer">
            <small>https://cocodataset.org/</small>
          </a>
          <p>COCO ist eines von mehreren trainierten Modeles welche in Projekten verwendet werden kann</p>
          <a href="https://www.tensorflow.org/js/models" target="_blank"
            rel="noopener noreferrer"><small>https://www.tensorflow.org/js/models</small></a>
          <aside class="notes">
          </aside>
        </section>
        <section data-auto-animate>
          <h3 data-id="title">COCO hat mehrere Funktionen:</h3>
          <ul>
            <li>Objekt-Segmentierung</li>
            <li>Erkennung im Kontext</li>
            <li>Segmentierung von Superpixel</li>
            <li>330K Bilder (>200K beschriftet)</li>
            <li>1,5 Millionen Objektinstanzen</li>
            <li>80 Objektkategorien</li>
            <li>91 Materialkategorien</li>
            <li>5 Beschriftungen pro Bild</li>
            <li>250.000 Menschen mit Schlüsselpunkten</li>
          </ul>
          <aside class="notes">
            A superpixel can be defined as a group of pixels that share common characteristics (like pixel intensity ).
            Superpixels are becoming useful in many Computer Vision and Image processing algorithms like Image
            Segmentation, Semantic labeling, Object detection and tracking.
          </aside>
        </section>
      </section>
      <section>
        <h2 data-id="title">Beispiel</h2>
        <p>
          Die Bilder der Kamera werden mit COCO analysiert und erstellt Layers im Video, welche erkannte Objekte
          markieren.
        </p>
        <a href="/demo1.html" target="_blank" rel="noreferrer noopener">
          <h3>
            Demo
          </h3>
        </a>
      </section>
      <section>
        <h2 data-id="title">Code Peak</h2>
        <pre data-id="code-animation">
          <code class="hljs" data-trim data-line-numbers="|1-3|24-28|39-43|49-54|80|83|89-100">
            <script type="text/template">
              import '@tensorflow/tfjs-backend-cpu';
              import '@tensorflow/tfjs-backend-webgl';
              import * as cocoSSD from '@tensorflow-models/coco-ssd';
              import Stats from 'stats.js';

              const stats = new Stats();
              stats.showPanel(1); // 0: fps, 1: ms, 2: mb, 3+: custom
              document.body.appendChild(stats.dom);
              stats.dom.style.left = 'auto';
              stats.dom.style.right = '0px';

              let model: cocoSSD.ObjectDetection | undefined;
              let video: HTMLVideoElement | undefined;
              let outputCanvas: HTMLCanvasElement | undefined;
              let outputContext: CanvasRenderingContext2D | null = null;

              const constraints = {
                audio: false,
                video: true,
              };

              window.onload = async function() {
                // load model
                // possible base convolutional neural networks models
                // - lite_mobilenet_v2
                // - mobilenet_v1
                // - mobilenet_v2
                model = await cocoSSD.load({base: 'mobilenet_v2'});

                // create dummy video element
                video = document.createElement('video');
                video.setAttribute('autoplay', 'autoplay');

                // get input/output canvas and their context
                outputCanvas = document.getElementById('output') as HTMLCanvasElement;
                outputContext = outputCanvas.getContext('2d');

                // request camera access
                navigator
                    .mediaDevices
                    .getUserMedia(constraints)
                    .then(handleSuccess)
                    .catch(handleError);
              };

              function handleSuccess(stream: MediaStream) {
                if (video === undefined || outputCanvas === undefined) return;

                video.addEventListener('loadeddata', () => {
                  // start painting to prevew and run the objectDetection on the input canvas
                  objectDetection();
                });
                // stream camera to the dummy video element
                video.srcObject = stream;

                const {width = 0, height = 0} = stream.getTracks()[0].getSettings();
                video.setAttribute('width', String(width));
                video.setAttribute('height', String(height));
                outputCanvas.setAttribute('width', String(width));
                outputCanvas.setAttribute('height', String(height));
              }

              function handleError(error: Error) {
                console.error(
                    'navigator.MediaDevices.getUserMedia error: ',
                    error.message,
                    error.name,
                );
              }


              const objectDetection = async function() {
                if (outputContext === null ||
                    model === undefined ||
                    video === undefined) return;

                stats.begin();

                // detect objects on input canvas
                const result = await model.detect(video);

                // draw input canvas to output canvas
                outputContext.drawImage(video, 0, 0, video.width, video.height);
                outputContext.font = '14px Arial';

                // console.log('number of detections: ', result.length);

                // draw detected objects to output canvas
                for (let i = 0; i < result.length; i++) {
                  outputContext.beginPath();
                  outputContext.rect(...result[i].bbox);
                  outputContext.lineWidth = 1;
                  outputContext.strokeStyle = 'red';
                  outputContext.fillStyle = 'red';
                  outputContext.stroke();
                  outputContext.fillText(
                      result[i].score.toFixed(3) + ' ' + result[i].class, result[i].bbox[0],
                    result[i].bbox[1] > 10 ? result[i].bbox[1] - 5 : 10,
                  );
                }
                stats.end();
                // start again on the next frame
                requestAnimationFrame(objectDetection);
              };
            </script>
          </code>
        </pre>
      </section>
      <section>
        <h2 data-id="title">Fragen?</h2>
      </section>
    </div>

  </div>
</body>

</html>